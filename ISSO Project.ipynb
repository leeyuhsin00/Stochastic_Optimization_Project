{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d03385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# define loss function\n",
    "def L2(theta):\n",
    "    u1 = theta[0]\n",
    "    u2 = theta[1]\n",
    "    \n",
    "    return 0.10*(1-u1)**2+0.90*(1-0.5*u1*u2-u2)**2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d2537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "mini = minimize(L2, np.array([1,1]))\n",
    "print(mini.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544209af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minimum\n",
    "minimum = np.array([1,2/3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f100119",
   "metadata": {},
   "source": [
    "Simple 'taking average' algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0990fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A with averaging\n",
    "def algorithm_A_avg(n_iters, replications, avg_no):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = np.mean([L2(theta) + np.random.normal(0,1) for k in range(avg_no)])\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # calculate new loss\n",
    "            new_loss = np.mean([L2(new_theta) + np.random.normal(0,1) for k in range(avg_no)])\n",
    "            # update\n",
    "            if new_loss < loss:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_avg, error_avg = algorithm_A_avg(1000, 40, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f24b76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(1001), loss_avg, label='loss')\n",
    "plt.plot(np.arange(1001), error_avg, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Simple Averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca152e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A with averaging\n",
    "def algorithm_A_avg_WB(n_iters, replications, avg_no):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = np.mean([L2(theta) + np.random.weibull(5) for k in range(avg_no)])\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # calculate new loss\n",
    "            new_loss = np.mean([L2(new_theta) + np.random.weibull(5) for k in range(avg_no)])\n",
    "            # update\n",
    "            if new_loss < loss:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_avg_WB, error_avg_WB = algorithm_A_avg_WB(1000, 40, 1000)\n",
    "\n",
    "plt.plot(np.arange(1001), loss_avg_WB, label='loss')\n",
    "plt.plot(np.arange(1001), error_avg_WB, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Simple Averaging with Weibull Noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A with averaging\n",
    "def algorithm_A_avg_Cauchy(n_iters, replications, avg_no):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = np.mean([L2(theta) + np.random.laplace(0,1) for k in range(avg_no)])\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # calculate new loss\n",
    "            new_loss = np.mean([L2(new_theta) + np.random.laplace(0,1) for k in range(avg_no)])\n",
    "            # update\n",
    "            if new_loss < loss:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37763728",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_avg_C, error_avg_C = algorithm_A_avg_Cauchy(1000, 40, 1000)\n",
    "\n",
    "plt.plot(np.arange(1001), loss_avg_C, label='loss')\n",
    "plt.plot(np.arange(1001), error_avg_C, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Simple Averaging with Laplace Noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7957761",
   "metadata": {},
   "source": [
    "Alexander et al: ISSARS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A with averaging\n",
    "def algorithm_A_ISSARS(n_iters, replications):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize n\n",
    "        n = 1\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = np.mean([L2(theta) + np.random.normal(0,1) for k in range(n)])\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # increase n\n",
    "            n += 1\n",
    "            # calculate new loss\n",
    "            new_loss = np.mean([L2(new_theta) + np.random.normal(0,1) for k in range(n)])\n",
    "            # update\n",
    "            if new_loss < loss:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ac71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ISSARS, error_ISSARS = algorithm_A_ISSARS(1000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1001), loss_ISSARS, label='loss')\n",
    "plt.plot(np.arange(1001), error_ISSARS, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('ISSARS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878d28f",
   "metadata": {},
   "source": [
    "Devroye stable errors:\n",
    "1. normal\n",
    "2. exponential\n",
    "3. Laplace (unstable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88655225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A\n",
    "def algorithm_A(n_iters, replications):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = L2(theta) + np.random.normal(0,1)\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # calculate new loss\n",
    "            new_loss = L2(new_theta) + np.random.normal(0,1)\n",
    "            # update\n",
    "            if new_loss < loss:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b23d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error = algorithm_A(1000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1001), loss, label='loss')\n",
    "plt.plot(np.arange(1001), error, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('Normal Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, error = algorithm_A(10000, 40)\n",
    "\n",
    "plt.plot(np.arange(10001), loss, label='loss')\n",
    "plt.plot(np.arange(10001), error, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Normal Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88313c",
   "metadata": {},
   "source": [
    "Normal error: Not enough evidence to show it converges??? SEE ALGORITHM B BELOW: converges but very slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945eda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A2\n",
    "def algorithm_A2(n_iters, replications):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = L2(theta) + np.random.exponential(1)\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # calculate new loss\n",
    "            new_loss = L2(new_theta) + np.random.exponential(1)\n",
    "            # update\n",
    "            if new_loss < loss:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a08308",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2, error2 = algorithm_A2(1000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58caaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1001), loss2, label='loss')\n",
    "plt.plot(np.arange(1001), error2, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('Exponential Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2deb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2, error2 = algorithm_A2(10000, 40)\n",
    "\n",
    "plt.plot(np.arange(10001), loss2, label='loss')\n",
    "plt.plot(np.arange(10001), error2, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('Exponential Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd5c9c",
   "metadata": {},
   "source": [
    "Exponential loss: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ab728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A3\n",
    "def algorithm_A3(n_iters, replications):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = L2(theta) + np.random.laplace(0,1)\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # calculate new loss\n",
    "            new_loss = L2(new_theta) + np.random.laplace(0,1)\n",
    "            # update\n",
    "            if new_loss < loss:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss3, error3 = algorithm_A3(1000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc222d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1001), loss3, label='loss')\n",
    "plt.plot(np.arange(1001), error3, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Laplace Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss3, error3 = algorithm_A3(10000, 40)\n",
    "\n",
    "plt.plot(np.arange(10001), loss3, label='loss')\n",
    "plt.plot(np.arange(10001), error3, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Laplace Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb4fa4",
   "metadata": {},
   "source": [
    "Laplace/Double Exponential error: Likely does not converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9520d09c",
   "metadata": {},
   "source": [
    "With positive threshold according to Spall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558445cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A with T\n",
    "def algorithm_AT(n_iters, replications):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    # define threshold\n",
    "    T = 1\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = L2(theta) + np.random.normal(0,1)\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # calculate new loss\n",
    "            new_loss = L2(new_theta) + np.random.normal(0,1)\n",
    "            # update\n",
    "            if new_loss < loss - T:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d56d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossT, errorT = algorithm_AT(1000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ffaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1001), lossT, label='loss')\n",
    "plt.plot(np.arange(1001), errorT, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('With Constant Threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9d3d3",
   "metadata": {},
   "source": [
    "Gelfand and Mitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da7944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm A with W\n",
    "def algorithm_AW(n_iters, replications):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 2*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = L2(theta) + np.random.normal(0,1)\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, randomly choose theta\n",
    "            new_theta = np.random.uniform(-3,3,size=2)\n",
    "            # calculate new loss\n",
    "            new_loss = L2(new_theta) + np.random.normal(0,1)\n",
    "            # decreasing variance of W\n",
    "            var = 0.3*1.002**j\n",
    "            # random variable W\n",
    "            W = np.abs(np.random.normal(0,var))\n",
    "            # update\n",
    "            if new_loss - loss + W < 0:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ddbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossW, errorW = algorithm_AW(1000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(1001), lossW, label='loss')\n",
    "plt.plot(np.arange(1001), errorW, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('With Random Threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2ffad",
   "metadata": {},
   "source": [
    "possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm B\n",
    "def algorithm_B(n_iters, replications):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 3*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = L2(theta) + np.random.normal(0,1)\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, generate random vector d\n",
    "            d = np.random.uniform(-0.5,0.5,size=2)\n",
    "            new_theta = theta + d\n",
    "            # calculate new loss\n",
    "            new_loss = L2(new_theta) + np.random.normal(0,1)\n",
    "            # update\n",
    "            if new_loss < loss:\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4499fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossB, errorB = algorithm_B(1000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03263b9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1001), lossB, label='loss')\n",
    "plt.plot(np.arange(1001), errorB, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('Algorithm B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossB, errorB = algorithm_B(10000, 40)\n",
    "\n",
    "plt.plot(np.arange(10001), lossB, label='loss')\n",
    "plt.plot(np.arange(10001), errorB, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Algorithm B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define algorithm B\n",
    "def algorithm_B_backtracking(n_iters, replications):\n",
    "    min_loss = 1.0\n",
    "    # loss matrix\n",
    "    loss_mat = np.zeros((replications, n_iters+1))  \n",
    "    # initialize error matrix\n",
    "    error_mat = np.zeros((replications, n_iters+1))\n",
    "    \n",
    "    for i in range(replications):\n",
    "        # initialize theta and calculate loss\n",
    "        theta = 3*np.ones(2)\n",
    "        # include error with stable noise\n",
    "        loss = L2(theta) + np.random.normal(0,1)\n",
    "        # update loss matrix without noise\n",
    "        loss_mat[i,0] = L2(theta) - min_loss\n",
    "        # update error matrix\n",
    "        error_mat[i,0] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "        # loop for n_iters (number of iterations)\n",
    "        for j in range(n_iters):\n",
    "            # using uniform distribution, generate random vector d\n",
    "            d = np.random.uniform(-0.5,0.5,size=2)\n",
    "            new_theta = theta + d\n",
    "            # at a small probability, take the minimum of 50 measurements\n",
    "            z = np.random.uniform(0,1)\n",
    "            if z < 0.1:\n",
    "                new_loss = np.min([L2(new_theta) + np.random.normal(0,1) for k in range(50)])\n",
    "            # calculate new loss\n",
    "            else: \n",
    "                new_loss = L2(new_theta) + np.random.normal(0,1)\n",
    "            \n",
    "            if new_loss < loss:\n",
    "                # update\n",
    "                theta = new_theta\n",
    "                loss = new_loss\n",
    "\n",
    "            # update loss matrix without noise\n",
    "            loss_mat[i,j+1] = L2(theta) - min_loss\n",
    "\n",
    "            # update error matrix\n",
    "            error_mat[i,j+1] = np.linalg.norm(theta-minimum)\n",
    "    \n",
    "    # normalize\n",
    "    return np.mean(loss_mat, axis=0)/(loss_mat[0,0]), np.mean(error_mat, axis=0)/error_mat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossB_b, errorB_b = algorithm_B_backtracking(1000, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1001), lossB_b, label='loss')\n",
    "plt.plot(np.arange(1001), errorB_b, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Algorithm B With Minimum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossB_b, errorB_b = algorithm_B_backtracking(10000, 40)\n",
    "\n",
    "plt.plot(np.arange(10001), lossB_b, label='loss')\n",
    "plt.plot(np.arange(10001), errorB_b, label='error')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Algorithm B With Minimum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def y_upper(theta):\n",
    "    return theta/2 + theta\n",
    "\n",
    "def y_lower(theta):\n",
    "    return theta/2 - theta\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "plt.plot(x, y_upper(x), color='blue', linestyle='dashed', label='W/ Noise')\n",
    "plt.plot(x, y_lower(x), color='blue', linestyle='dashed')\n",
    "plt.plot(x, np.zeros(100), color='black', linestyle='dotted')\n",
    "plt.plot(x, x/2, label='W/o Noise')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,100)\n",
    "plt.plot(x, x**2, label='true Y')\n",
    "plt.plot(x, x**2+3, color = 'red', linestyle = 'dashed', label='measured Y range')\n",
    "plt.plot(x, x**2-3, color = 'red', linestyle = 'dashed')\n",
    "plt.plot(-1, 1**2,'go')\n",
    "plt.axhline(y = 1, color = 'g', linestyle = 'dotted') \n",
    "plt.plot(-1, 1-3, 'bo')\n",
    "plt.axhline(y = -2, color = 'b', linestyle = 'dotted') \n",
    "plt.legend()\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.title('Range of Y with errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd955d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
